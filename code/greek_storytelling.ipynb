{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCPb0b2zMlMp",
        "outputId": "e8380c6d-5855-421f-e127-bcf173f8dda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPMhDFdJMuYr",
        "outputId": "2291539d-8ae8-48c5-f196-189750ab4d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.2\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.11.2\n",
            "    Uninstalling tokenizers-0.11.2:\n",
            "      Successfully uninstalled tokenizers-0.11.2\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bhRjLrhEMuar"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vd93c2iQMuc3"
      },
      "outputs": [],
      "source": [
        "class BPE_token(object):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer(BPE())\n",
        "        # self.tokenizer.normalizer = Sequence([\n",
        "        #     NFKC()\n",
        "        # ])\n",
        "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
        "        self.tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "    def bpe_train(self, paths):\n",
        "        trainer = BpeTrainer(vocab_size=25000, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
        "            \"<s>\",\n",
        "            \"<pad>\",\n",
        "            \"</s>\",\n",
        "            \"<unk>\",\n",
        "            \"<mask>\"\n",
        "        ])\n",
        "        self.tokenizer.train(trainer = trainer, files = paths)\n",
        "\n",
        "    def save_tokenizer(self, location, prefix=None):\n",
        "        if not os.path.exists(location):\n",
        "            os.makedirs(location)\n",
        "        self.tokenizer.model.save(location, prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqAqxA4uMufA",
        "outputId": "eeb832dc-fc7d-46f5-e5bc-d8e57f620c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gdrive/MyDrive/project/data/corpus_processed.txt',\n",
              " 'gdrive/MyDrive/project/data/paidika_paramythia_processed.txt',\n",
              " 'gdrive/MyDrive/project/data/paramithia_processed.txt',\n",
              " 'gdrive/MyDrive/project/data/paramithia2_processed.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "# the folder 'text' contains all the files\n",
        "paths = [str(x) for x in Path(\"./gdrive/MyDrive/project/data/\").glob(\"**/*processed.txt\")]\n",
        "\n",
        "paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JPmvadF7MuhD"
      },
      "outputs": [],
      "source": [
        "tokenizer = BPE_token()\n",
        "# train the tokenizer model\n",
        "tokenizer.bpe_train(paths)\n",
        "# saving the tokenized data in our specified folder \n",
        "save_path = 'gdrive/MyDrive/project/model'\n",
        "tokenizer.save_tokenizer(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSlUXgA-Mujp",
        "outputId": "c17d9b63-02f1-48b4-b1ea-9ba90d6c7356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file gdrive/MyDrive/project/model/config.json not found\n"
          ]
        }
      ],
      "source": [
        "# loading tokenizer from the saved model path\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
        "tokenizer.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.vocab_size,\n",
        "  bos_token_id=tokenizer.bos_token_id,\n",
        "  eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "# creating the model\n",
        "model = TFGPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XlKTyHFEMumA"
      },
      "outputs": [],
      "source": [
        "single_string = ''\n",
        "for filename in paths:\n",
        "  with open(filename, \"r\", encoding='utf-8') as f:\n",
        "   x = f.read()\n",
        "  single_string += x + tokenizer.eos_token\n",
        "string_tokenized = tokenizer.encode(single_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UoPpxJK_Muot"
      },
      "outputs": [],
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
        "  examples.append(string_tokenized[i:i + block_size])\n",
        "inputs, labels = [], []\n",
        "for ex in examples:\n",
        "  inputs.append(ex[:-1])\n",
        "  labels.append(ex[1:])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "da_mHEdnMusR"
      },
      "outputs": [],
      "source": [
        "# defining our optimizer\n",
        "\n",
        "# clipnorm:\n",
        "# Gradient norm scaling involves changing the derivatives of the loss function \n",
        "# to have a given vector norm when the L2 vector norm (sum of the squared values) \n",
        "# of the gradient vector exceeds a threshold value. For example, we could specify \n",
        "# a norm of 1.0, meaning that if the vector norm for a gradient exceeds 1.0, then\n",
        "#  the values in the vector will be rescaled so that the norm of the vector equals 1.0.\n",
        "\n",
        "# epsilon: avoid zero division\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=6e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# compiling the model\n",
        "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1,\n",
        "#     save_best_only=True, mode='auto', period=1)"
      ],
      "metadata": {
        "id": "px2pHCuGKJDG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U0py_x0jMu-d"
      },
      "outputs": [],
      "source": [
        "# num_epoch = 30\n",
        "# history = model.fit(dataset, epochs=num_epoch)\n",
        "\n",
        "# save_path = 'gdrive/MyDrive/project/model'\n",
        "# model.save_pretrained(save_directory = save_path, save_config = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 30\n",
        "checkpoint_filepath = 'gdrive/MyDrive/project/model/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath)\n",
        "\n",
        "# Model weights are saved at the end of every epoch, if it's the best seen\n",
        "# so far.\n",
        "# history = model.fit(dataset, epochs=num_epoch, callbacks=[model_checkpoint_callback])\n",
        "\n",
        "# The model weights (that are considered the best) are loaded into the model.\n",
        "#model.load_weights(checkpoint_filepath)\n",
        "\n",
        "\n",
        "#save_path = 'gdrive/MyDrive/project/model'\n",
        "#model.save_pretrained(save_directory = save_path, save_config = True)"
      ],
      "metadata": {
        "id": "41xCiNnl51TP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = model.load_weights(checkpoint_filepath) #trained for 12 epochs"
      ],
      "metadata": {
        "id": "KGGYi_vwJCGi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 30\n",
        "checkpoint_filepath = 'gdrive/MyDrive/project/model/checkpoint'\n",
        "\n",
        "history2 = model.fit(dataset, epochs=num_epoch, callbacks=[model_checkpoint_callback])\n",
        "save_path = 'gdrive/MyDrive/project/model'\n",
        "model.save_pretrained(save_directory = save_path, save_config = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdvWFGrtJo7-",
        "outputId": "5bdb74dd-2a23-4fd6-85b0-51de301b7af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            " 56/322 [====>.........................] - ETA: 1:13:21 - loss: 4.3249 - logits_loss: 4.3249 - logits_accuracy: 0.3006 - past_key_values_1_accuracy: 0.0015 - past_key_values_2_accuracy: 0.0018 - past_key_values_3_accuracy: 0.0019 - past_key_values_4_accuracy: 0.0016 - past_key_values_5_accuracy: 0.0018 - past_key_values_6_accuracy: 0.0017 - past_key_values_7_accuracy: 0.0018 - past_key_values_8_accuracy: 0.0015 - past_key_values_9_accuracy: 0.0015 - past_key_values_10_accuracy: 0.0017 - past_key_values_11_accuracy: 0.0019 - past_key_values_12_accuracy: 0.0018"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history2)"
      ],
      "metadata": {
        "id": "rIYqKHQZXO67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sess = gpt2.start_tf_sess()\n",
        "\n",
        "# gpt2.finetune(sess,\n",
        "#     file_name,\n",
        "#     model_name=model_name,\n",
        "#     checkpoint_dir=checkpoint_dir,\n",
        "#     run_name=run_name,\n",
        "#     steps=25,\n",
        "# )"
      ],
      "metadata": {
        "id": "y5Gr0IeoLdEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3u24xdlsNi3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43630b3d-c125-4e17-b487-674b77a18802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Μια φορά και έναν καιρό,  ένας βασιλιάς που είχε έρθει ο πατέρας του.Ο πρίγκιπας ήταν πολύ μακριά από το σπίτι της είπε: Δεν είναι ότι δεν μπορούσε να τον πατέρα μου;Και τι θα σε μια μέρα η αλεπού για την άλλη ημέρα με τα μάτια σου! Θα γίνει καλά κι εμείς καλύτερα... Το βράδυ στο παλάτι των παιδιών μας πει ένα μικρό πρίγκιπα βασιλιά στον κήπο τους έλεγε πως οι άνθρωποι αλλά αν είμαι τόσο όμορφα χρόνια μαζί σας πω αυτό όμως δε πρέπει ν’ όλα αυτά τις δυο παιδιά\n"
          ]
        }
      ],
      "source": [
        "text = \"Μια φορά και έναν καιρό, \"\n",
        "# encoding the input text\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "\n",
        "# getting out output\n",
        "beam_output = model.generate(\n",
        "  input_ids,\n",
        "  max_length = 100,\n",
        "  num_beams = 10,\n",
        "  temperature = 0.7,\n",
        "  no_repeat_ngram_size=1,\n",
        "  num_return_sequences=10,\n",
        "  repetition_penalty=1.5,\n",
        "  skip_special_tokens = True,\n",
        "  clean_up_tokenization = True,\n",
        "  early_stopping = True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRQBq0g5Nir7"
      },
      "outputs": [],
      "source": [
        "text = \"Σε μία μακρινή \"\n",
        "# encoding the input text\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "# getting out output\n",
        "beam_output = model.generate(\n",
        "  input_ids,\n",
        "  max_length = 500,\n",
        "  num_beams = 10,\n",
        "  #temperature = 0.1,\n",
        "  no_repeat_ngram_size=1,\n",
        "  num_return_sequences=8,\n",
        "  repetition_penalty=1.5,\n",
        "  skip_special_tokens = True,\n",
        "  clean_up_tokenization = True,\n",
        "  early_stopping = True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5iK2tJWNig1"
      },
      "outputs": [],
      "source": [
        "text = \"Ο νεαρός βοσκός ήταν πολύ στεναχωρημένος\"\n",
        "# encoding the input text\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "# getting out output\n",
        "beam_output = model.generate(\n",
        "  input_ids,\n",
        "  max_length = 500,\n",
        "  num_beams = 10,\n",
        "  #temperature = 0.1,\n",
        "  no_repeat_ngram_size=1,\n",
        "  num_return_sequences=10,\n",
        "  repetition_penalty=1.5,\n",
        "  skip_special_tokens = True,\n",
        "  clean_up_tokenization = True,\n",
        "  early_stopping = True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "greek_storytelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}