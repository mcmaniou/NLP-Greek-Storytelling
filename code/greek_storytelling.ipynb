{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCPb0b2zMlMp",
        "outputId": "9bb7342b-0787-4797-edcc-d0f3755a189b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download the necessary libraries"
      ],
      "metadata": {
        "id": "Y9Bvb7jCOpeA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPMhDFdJMuYr",
        "outputId": "06e280fe-dfcc-4657-8a79-ed571d63b58b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 4.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.2\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 71.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.11.2\n",
            "    Uninstalling tokenizers-0.11.2:\n",
            "      Successfully uninstalled tokenizers-0.11.2\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import libraries"
      ],
      "metadata": {
        "id": "wtApfDS6O2af"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhRjLrhEMuar"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class with all the tokenization steps to process all the input files"
      ],
      "metadata": {
        "id": "n-K0ZfB2O7Yq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd93c2iQMuc3"
      },
      "outputs": [],
      "source": [
        "class BPE_token(object):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer(BPE())\n",
        "        # self.tokenizer.normalizer = Sequence([\n",
        "        #     NFKC()\n",
        "        # ])\n",
        "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
        "        self.tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "    def bpe_train(self, paths):\n",
        "        trainer = BpeTrainer(vocab_size=25000, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
        "            \"<s>\",\n",
        "            \"<pad>\",\n",
        "            \"</s>\",\n",
        "            \"<unk>\",\n",
        "            \"<mask>\"\n",
        "        ])\n",
        "        self.tokenizer.train(trainer = trainer, files = paths)\n",
        "\n",
        "    def save_tokenizer(self, location, prefix=None):\n",
        "        if not os.path.exists(location):\n",
        "            os.makedirs(location)\n",
        "        self.tokenizer.model.save(location, prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read all the input files "
      ],
      "metadata": {
        "id": "H5Pj0jI1Pdbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqAqxA4uMufA",
        "outputId": "c20c696a-8fcc-4de1-da19-985167ff1352"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['gdrive/MyDrive/project/data/corpus_processed.txt',\n",
              " 'gdrive/MyDrive/project/data/paidika_paramythia_processed.txt',\n",
              " 'gdrive/MyDrive/project/data/paramithia_processed.txt',\n",
              " 'gdrive/MyDrive/project/data/paramithia2_processed.txt']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# the folder 'text' contains all the files\n",
        "paths = [str(x) for x in Path(\"./gdrive/MyDrive/project/data/\").glob(\"**/*processed.txt\")]\n",
        "\n",
        "paths"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## perform tokenization "
      ],
      "metadata": {
        "id": "SzJgUQBkPk7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPmvadF7MuhD"
      },
      "outputs": [],
      "source": [
        "tokenizer = BPE_token()\n",
        "# train the tokenizer model\n",
        "tokenizer.bpe_train(paths)\n",
        "# saving the tokenized data in our specified folder \n",
        "save_path = 'gdrive/MyDrive/project/model'\n",
        "tokenizer.save_tokenizer(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load and config GPT-2"
      ],
      "metadata": {
        "id": "v0JbcZiQPsAl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSlUXgA-Mujp",
        "outputId": "b8f90131-a5df-46b9-d081-d6960d7081fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "file gdrive/MyDrive/project/model/config.json not found\n"
          ]
        }
      ],
      "source": [
        "# loading tokenizer from the saved model path\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
        "tokenizer.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.vocab_size,\n",
        "  bos_token_id=tokenizer.bos_token_id,\n",
        "  eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "# creating the model\n",
        "model = TFGPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## merge all the fairytales in one string"
      ],
      "metadata": {
        "id": "q3bM8Gb2Px-8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlKTyHFEMumA"
      },
      "outputs": [],
      "source": [
        "single_string = ''\n",
        "for filename in paths:\n",
        "  with open(filename, \"r\", encoding='utf-8') as f:\n",
        "   x = f.read()\n",
        "  single_string += x + tokenizer.eos_token\n",
        "string_tokenized = tokenizer.encode(single_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create the input dataset for the model"
      ],
      "metadata": {
        "id": "GU0XHzKBP7h3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoPpxJK_Muot"
      },
      "outputs": [],
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
        "  examples.append(string_tokenized[i:i + block_size])\n",
        "inputs, labels = [], []\n",
        "for ex in examples:\n",
        "  inputs.append(ex[:-1])\n",
        "  labels.append(ex[1:])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## define the training hyperparameters"
      ],
      "metadata": {
        "id": "aXe7YkjyQEkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da_mHEdnMusR"
      },
      "outputs": [],
      "source": [
        "# defining our optimizer\n",
        "\n",
        "# clipnorm:\n",
        "# Gradient norm scaling involves changing the derivatives of the loss function \n",
        "# to have a given vector norm when the L2 vector norm (sum of the squared values) \n",
        "# of the gradient vector exceeds a threshold value. For example, we could specify \n",
        "# a norm of 1.0, meaning that if the vector norm for a gradient exceeds 1.0, then\n",
        "#  the values in the vector will be rescaled so that the norm of the vector equals 1.0.\n",
        "\n",
        "# epsilon: avoid zero division\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=6e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# compiling the model\n",
        "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train the model"
      ],
      "metadata": {
        "id": "WHoh5lJBQzLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41xCiNnl51TP"
      },
      "outputs": [],
      "source": [
        "num_epoch = 35\n",
        "checkpoint_filepath = 'gdrive/MyDrive/project/model/checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath)\n",
        "\n",
        "# Model weights are saved at the end of every epoch, if it's the best seen\n",
        "# so far.\n",
        "history = model.fit(dataset, epochs=num_epoch, callbacks=[model_checkpoint_callback])\n",
        "\n",
        "# The model weights (that are considered the best) are loaded into the model.\n",
        "# model.load_weights(checkpoint_filepath)\n",
        "\n",
        "\n",
        "#save_path = 'gdrive/MyDrive/project/model'\n",
        "model.save_pretrained(save_directory = save_path, save_config = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load weights and continue the training process"
      ],
      "metadata": {
        "id": "SsXK30Z8Q3iI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGGYi_vwJCGi"
      },
      "outputs": [],
      "source": [
        " \n",
        "loaded_model = model.load_weights(checkpoint_filepath)\n",
        "num_epoch = 5\n",
        "checkpoint_filepath = 'gdrive/MyDrive/project/model/checkpoint'\n",
        "\n",
        "history2 = model.fit(dataset, epochs=num_epoch, callbacks=[model_checkpoint_callback])\n",
        "save_path = 'gdrive/MyDrive/project/model'\n",
        "model.save_pretrained(save_directory = save_path, save_config = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generate short stories"
      ],
      "metadata": {
        "id": "YOXooG8KRJLp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t5iK2tJWNig1",
        "outputId": "3b93d602-48d8-4be0-e35b-23122a80a7e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ο νεαρός βοσκός ήταν πολύ στεναχωρημένος και η τρομάρα του είπαν ότι είχε έρθει καλά από τη χαρά με εδώ!Σαν έγινε ο Παπουτσωμένος Γάτος, στο μεταξύ τους δέχτηκε.Όταν κάποιος βασιλιάς μπήκε στην πόρτα αυτή την τύχη αυτό να μπει μέσα σ’ ένα λιβάδι δίπλα σε έναν μεγάλο διάδρομο όπου ζούσε ένας κακός άνθρωπος τις τρεις γιούς της κόρης.\"Τι ψάχνεις; ρώτησαν μαζί μας;.Τα ποντίκια απάντησαν: Να μου τα μάγια κοιτάχτηκαν γύρω μ΄ όλο τον κόσμο ή εσύ!, είπε αποφασιστικά το κορίτσι που θα έλεγες αν δούμε τι κάνεις για πες σας δώσω μια φοβερή έκπληξη!!Την ίδια όμως ακόμη μεγαλύτερο γιο σου;\"\"Όχι απάντησε ειρωνικά.– Αν θέλεις!.Ναι λοιπόν ποιο δώρο μην πάω εκεί κοντά στη μέση ενός κήπου δεν πειράζει όσο πιο πέρα δώθε κάτω – όπως κάνουν μία φορά δε γυρεύεις πάρα πολλά χρόνια αλλά όταν έχει αξία...Έτσι ήρθε μόνο αυτός έμεινε κλεισμένος στον πύργο μακριά κι άλλες χώρες....Μια μέρα ξέρεις πόσο ευτυχισμένη είναι ανίκανος είμαι μοναχούφιας λέει· γιατί είσαι πλούσιος κύριος Γιώργος μεγάλωσε μπροστά στους άλλους εργάτες περίμενε καθόλου μεγάλη πόλη!!! Εκεί μαζεύτηκαν πολλές ημέρες οι άνθρωποι πάνω στις πόρτες όλων των ανθρώπων χωρίς όνομα  Όλα πήγαιναν πουθενά αγάπη τότε \"Να περιμένουμε κανέναν τόπο χωριό ανάμεσα στα χωράφια μιας πανύψηλου σπιτιού λένε όλοι θέλουν νείθούν αρκετά σημανοί (ο βασιλιά αναρωτιόταν πως ας κάνουμε κανένα τρίτο πρωί μόλις άκουσε όλα αυτά πριν χαθεί πραγματικά ήθελαν κάποιες μέρες πέρασαν δυο πριγκιπόπουλα καθώς γύριζε ποτέ ξανά πρώτος κυρ Θωμάς δια μαγείας έκαναν όσα είχαν μάθει τόσες μεγάλες ρίζες ‘δω ώσπου νύχτωσε σα πατέρα αυτού ως τιμήνα δουν εμάς μέχρι αργά σαν αστραπή ενώ σήμερα Ο πονηρός γάτος πήγε χαμένο χρόνο βρήκαν κάποιον τρόπο επειδή πάνε κάθε είδους φλουριά ώστε πότε πέρασε μερικούς ανθρώπουςΤο λιοντάρι πάλι ζήτησε εγώ έχω καταφέρει ούτε καν καταλάβει ποια καρδιά Δεν ξέρετε τίποτα) Όταν έφτασε μακρυά κυφων οικογένειατης ζωήςτου κόσμου’.Μετά πήρε άλλη ημέρα μάλιστα θέλει τίποτε άλλο γουρουνάκι άρχισε αμέσως μετά έφυγε τρέχονταςστο πρώτο καράβι όλες αυτές βοηθήσει κανείς άλλος μάγειρας αναστέναξε γεμάτος προσοχή τόσα χρήματα…Μόλις τελείωσε έκπληκτος έκανε καταφύγιο ακόμα αγοράσει λίγο χορταράκι\" έλεγε:– Και μάλλον έχουμε δει κανένας υπηρέτης είδε πόσα ζαχαρωτά\", αποκρίθηκε θαρρετά αφού έβαλε αρκετή ώρα έτσι σταμάτησαν καλή κακή γοργόνα μα σιγά σιγάσιγά πάει γιος παραδείώσει τόσο πλούσιο καφέ ανοιχτό ολόκληρο φόβο τούτοΗ κυρία κατσίκα κοίταξε παραξενεμένοςσαν επιτέλους πρώτη χρονιά σκεφτόταν ποιος μπορεί νανσήλλήτό ψωμί Για δεύτερη κόρη χαμογέλασε γελώντας:\"Τώρα πια….Και ζήσαν αυτοί γα Μπιζέλης πήγαινε μόνον όλη µέρασεςμένα ρούχα κάλυζεται βαθιά ανάσα..Τότε έγιναν καλύτεροι δύο μεγαλύτεροι αδερφοίρια δουλειά γιαν μήπως γίνονται καινούρια σχέδια γεμάτη περιουσία εέκικά έχοντας ακούσει εμένα;–Όσο περνούσαν περισσότερα πράγματα’, τώρα τί συμβαίνει\n"
          ]
        }
      ],
      "source": [
        "text = \"Ο νεαρός βοσκός ήταν πολύ στεναχωρημένος\"\n",
        "# encoding the input text\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "# getting out output\n",
        "beam_output = model.generate(\n",
        "  input_ids,\n",
        "  max_length = 500,\n",
        "  num_beams = 10,\n",
        "  #temperature = 0.1,\n",
        "  no_repeat_ngram_size=1,\n",
        "  num_return_sequences=10,\n",
        "  repetition_penalty=1.5,\n",
        "  skip_special_tokens = True,\n",
        "  clean_up_tokenization = True,\n",
        "  early_stopping = True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(beam_output[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "greek_storytelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}