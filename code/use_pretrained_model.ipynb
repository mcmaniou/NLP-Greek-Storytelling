{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "use_pretrained_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRm2vB2p9rgZ",
        "outputId": "a25999c0-b75c-46fe-836e-6b5edc7e21c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkr89Nag-Che",
        "outputId": "0a071e47-4a07-47ef-f02c-552f3cf4ddc4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 12.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.2\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 399 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.11.2\n",
            "    Uninstalling tokenizers-0.11.2:\n",
            "      Successfully uninstalled tokenizers-0.11.2\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n"
      ],
      "metadata": {
        "id": "FSLOUlb2-Cj6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = 'gdrive/MyDrive/project/model'"
      ],
      "metadata": {
        "id": "BVRiDmTQ-CmO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading tokenizer from the saved model path\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n",
        "tokenizer.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})\n",
        "\n",
        "# loading pretrained model\n",
        "model = TFGPT2LMHeadModel.from_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjWSNnzg-Coe",
        "outputId": "37702ad6-c6d0-40f8-9e44-7be4a8d265a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gdrive/MyDrive/project/model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Μια φορά και έναν καιρό, \"\n",
        "# encoding the input text\n",
        "input_ids = tokenizer.encode(text, return_tensors='tf')\n",
        "\n",
        "# getting out output\n",
        "beam_output = model.generate(\n",
        "  input_ids,\n",
        "  max_length = 100,\n",
        "  num_beams = 10,\n",
        "  temperature = 0.7,\n",
        "  no_repeat_ngram_size=1,\n",
        "  num_return_sequences=10,\n",
        "  repetition_penalty=1.5,\n",
        "  skip_special_tokens = True,\n",
        "  clean_up_tokenization = True,\n",
        "  early_stopping = True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHlhxFhQ-Cq2",
        "outputId": "ad4215d9-a5f1-4b7a-8e5e-63dfbcb30c5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 2 (first `eos_token_id`) to generate sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(beam_output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U_Z9keu-CtH",
        "outputId": "57235f62-39a9-443e-e3d4-5ebb56a44d78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Μια φορά και έναν καιρό,  είχε το σπίτι του. Ο βασιλιάς από την πόρτα της είπε ο πρίγκιπας που ήταν η πριγκίπισσα με τα παιδιά τους να μην μια μέρα δεν θα είναι ένα μικρό ημέρα στο παλάτι για τον κάνει στην φωλιά μου!Ο πατέρας: Η μαμά σου; Το δάσος ότι είχαν έχει πολύ καλά κι’ αυτό τις μάτια μας σε τη ώρα στον άλλη ρώτησε τι μπορούσε οι πόδια...Η μικρός βασιλιά στα δυο ίδια όταν είδε λίγο τόσο στιγμή όμως πως πρέπει όλα αυτά αλλά αν πήγε στη μικρή\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rQrTg3m1-Cvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "24MyEvGc-CyL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}